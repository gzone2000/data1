{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 파라미터 ========\n",
    "training_steps = 22000\n",
    "display_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플수: 10\n"
     ]
    }
   ],
   "source": [
    "# ======== 학습데이터 ========\n",
    "# 학습시간\n",
    "X1 = np.array([54, 8, 30, 24, 46, 12, 20, 37, 40, 48])\n",
    "# 해외거주(월)\n",
    "X2 = [12, 0, 12, 15, 12, 0, 36, 12, 12, 24]\n",
    "# 토익점수\n",
    "Y = np.array([800, 320, 600, 630, 700, 680, 730, 720, 700, 920])\n",
    "# 샘플 갯수\n",
    "n_samples = X1.shape[0]\n",
    "\n",
    "print(\"샘플수: %i\" % (n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'weight:0' shape=() dtype=float32, numpy=0.29719105>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======== W, B 초기값 설정 =========\n",
    "W1 = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "W2 = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Multiple Linear Regression 에서 학습될 가설 ========\n",
    "# Linear regression (Wx + b).\n",
    "def linear_regression(x1, x2):\n",
    "    return W1 * x1 + W2 * x2 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Multiple Linear Regression 에서 학습될 가설의 Cost Function ========\n",
    "# Mean square error.\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Gradient Descent Algorithm 에서 Step ========\n",
    "learning_rate = 0.0006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 텐서플로우에 내장된 GradientDescentOptimizer ========\n",
    "# Stochastic Gradient Descent Optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== GradientDescentOptimizer ========\n",
    "# Optimization process. \n",
    "def run_optimization():\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation. : 자동 미분\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_regression(X1, X2)\n",
    "        cost = mean_square(pred, Y)\n",
    "        #pred = W1 * X1 + W2 * X2 + b\n",
    "        #cost = tf.reduce_mean(tf.square(pred - Y)) \n",
    "\n",
    "    # Compute gradients.\n",
    "    weight1, weight2, bias = g.gradient(cost, [W1, W2, b])\n",
    "\n",
    "    # Update W and b following gradients.\n",
    "    W1.assign_sub(learning_rate * weight1)\n",
    "    W2.assign_sub(learning_rate * weight2)\n",
    "    b.assign_sub(learning_rate * bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1000, cost: 25538.261719, W1: 13.007554, W2: 10.295521, b: 71.934021\n",
      "step: 2000, cost: 19696.972656, W1: 11.784702, W2: 9.510249, b: 131.028885\n",
      "step: 3000, cost: 15723.226562, W1: 10.776104, W2: 8.862564, b: 179.769958\n",
      "step: 4000, cost: 13019.922852, W1: 9.944212, W2: 8.328352, b: 219.971558\n",
      "step: 5000, cost: 11180.912109, W1: 9.258076, W2: 7.887741, b: 253.129486\n",
      "step: 6000, cost: 9929.844727, W1: 8.692148, W2: 7.524324, b: 280.478180\n",
      "step: 7000, cost: 9078.768555, W1: 8.225379, W2: 7.224581, b: 303.035095\n",
      "step: 8000, cost: 8499.791992, W1: 7.840389, W2: 6.977355, b: 321.639862\n",
      "step: 9000, cost: 8105.918945, W1: 7.522852, W2: 6.773444, b: 336.985046\n",
      "step: 10000, cost: 7837.975098, W1: 7.260949, W2: 6.605258, b: 349.641663\n",
      "step: 11000, cost: 7655.692871, W1: 7.044930, W2: 6.466540, b: 360.080872\n",
      "step: 12000, cost: 7531.690430, W1: 6.866762, W2: 6.352127, b: 368.690948\n",
      "step: 13000, cost: 7447.333008, W1: 6.719808, W2: 6.257758, b: 375.792542\n",
      "step: 14000, cost: 7389.946289, W1: 6.598603, W2: 6.179924, b: 381.649902\n",
      "step: 15000, cost: 7350.906250, W1: 6.498630, W2: 6.115725, b: 386.481140\n",
      "step: 16000, cost: 7324.343750, W1: 6.416173, W2: 6.062775, b: 390.465912\n",
      "step: 17000, cost: 7306.278809, W1: 6.348164, W2: 6.019102, b: 393.752441\n",
      "step: 18000, cost: 7293.988281, W1: 6.292068, W2: 5.983081, b: 396.463257\n",
      "step: 19000, cost: 7285.627441, W1: 6.245803, W2: 5.953370, b: 398.699066\n",
      "step: 20000, cost: 7279.941406, W1: 6.207648, W2: 5.928869, b: 400.542908\n",
      "step: 21000, cost: 7276.069336, W1: 6.176177, W2: 5.908658, b: 402.063843\n",
      "step: 22000, cost: 7273.435059, W1: 6.150216, W2: 5.891988, b: 403.318359\n"
     ]
    }
   ],
   "source": [
    "#run_optimization()\n",
    "#'''\n",
    "# Run training for the given number of steps.\n",
    "for step in range(1, training_steps + 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization()\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = linear_regression(X1, X2)\n",
    "        cost = mean_square(pred, Y)\n",
    "        print(\"step: %i, cost: %f, W1: %f, W2: %f, b: %f\" % (step, cost, W1.numpy(), W2.numpy(), b.numpy()))\n",
    "        #print(\"예측값 : {}\".format(pred.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30시간, 10개월 해외거주: 646.744751 점\n",
      "2시간, 24개월 해외거주: 415.102325 점\n"
     ]
    }
   ],
   "source": [
    "# ======== 학습된 우리의 프로그램에 예측 문의 ========\n",
    "\n",
    "# 30 시간 공부하고 10개월 해외에서 거주했을 경우 토익점수 예측\n",
    "print (\"30시간, 10개월 해외거주: %f 점\" % (linear_regression(30,10).numpy()))\n",
    "# 30 시간 공부하고 10개월 해외에서 거주했을 경우 토익점수 예측\n",
    "print (\"2시간, 24개월 해외거주: %f 점\" % (linear_regression(2,24).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
