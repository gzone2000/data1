{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 파라미터 ========\n",
    "training_steps = 40000\n",
    "display_step = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54., 12.],\n",
       "       [ 8.,  0.],\n",
       "       [30., 12.],\n",
       "       [24., 15.],\n",
       "       [46., 12.],\n",
       "       [12.,  0.],\n",
       "       [20., 36.],\n",
       "       [37., 12.],\n",
       "       [40., 12.],\n",
       "       [48., 24.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======== 학습데이터 ========\n",
    "# 학습시간, 해외거주\n",
    "data = np.array( [\n",
    "    [54, 8, 30, 24, 46, 12, 20, 37, 40, 48],  # 학습시간\n",
    "    [12, 0, 12, 15, 12, 0, 36, 12, 12, 24]  # 해외거주\n",
    "], dtype=np.float32)\n",
    "\n",
    "# 토익점수\n",
    "Y = np.array([800, 320, 600, 630, 700, 680, 730, 720, 700, 920])\n",
    "\n",
    "#  배열 위치 변환\n",
    "X = data[:].T\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.18287762], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======== W, B 초기값 설정 =========\n",
    "W = tf.Variable(tf.random.normal((2,1)))\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Multiple Linear Regression 에서 학습될 가설 ========\n",
    "# Linear regression (Wx + b).\n",
    "def linear_regression(X):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Multiple Linear Regression 에서 학습될 가설의 Cost Function ========\n",
    "# Mean square error.\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Gradient Descent Algorithm 에서 Step ========\n",
    "learning_rate = 0.0006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 텐서플로우에 내장된 GradientDescentOptimizer ========\n",
    "# Stochastic Gradient Descent Optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== GradientDescentOptimizer ========\n",
    "# Optimization process. \n",
    "def run_optimization():\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation. : 자동 미분\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_regression(X)\n",
    "        cost = mean_square(pred, Y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    # gradients = g.gradient(cost, [W, b])\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    # optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    \n",
    "    weight, bias = g.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * weight)\n",
    "    b.assign_sub(learning_rate * bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2000, cost: 55575.933594, W1: 9.526249, W2: 6.117411, b: 219.639130\n",
      "step: 4000, cost: 37291.550781, W1: 6.480593, W2: 4.161601, b: 366.822021\n",
      "step: 6000, cost: 28829.664062, W1: 4.408663, W2: 2.831083, b: 466.949127\n",
      "step: 8000, cost: 24913.589844, W1: 2.999158, W2: 1.925950, b: 535.064148\n",
      "step: 10000, cost: 23101.265625, W1: 2.040289, W2: 1.310199, b: 581.402039\n",
      "step: 12000, cost: 22262.539062, W1: 1.387982, W2: 0.891312, b: 612.925049\n",
      "step: 14000, cost: 21874.386719, W1: 0.944229, W2: 0.606349, b: 634.369629\n",
      "step: 16000, cost: 21694.755859, W1: 0.642351, W2: 0.412494, b: 648.958069\n",
      "step: 18000, cost: 21611.617188, W1: 0.436984, W2: 0.280615, b: 658.882568\n",
      "step: 20000, cost: 21573.144531, W1: 0.297278, W2: 0.190900, b: 665.633911\n",
      "step: 22000, cost: 21555.341797, W1: 0.202247, W2: 0.129876, b: 670.226318\n",
      "step: 24000, cost: 21547.099609, W1: 0.137602, W2: 0.088363, b: 673.350342\n",
      "step: 26000, cost: 21543.287109, W1: 0.093612, W2: 0.060114, b: 675.476135\n",
      "step: 28000, cost: 21541.523438, W1: 0.063695, W2: 0.040904, b: 676.921875\n",
      "step: 30000, cost: 21540.705078, W1: 0.043316, W2: 0.027817, b: 677.906738\n",
      "step: 32000, cost: 21540.324219, W1: 0.029416, W2: 0.018889, b: 678.578430\n",
      "step: 34000, cost: 21540.152344, W1: 0.020222, W2: 0.012986, b: 679.022766\n",
      "step: 36000, cost: 21540.072266, W1: 0.013893, W2: 0.008921, b: 679.328613\n",
      "step: 38000, cost: 21540.033203, W1: 0.009336, W2: 0.005994, b: 679.548828\n",
      "step: 40000, cost: 21540.015625, W1: 0.006810, W2: 0.004373, b: 679.670898\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step in range(1, training_steps + 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization()\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = linear_regression(X)\n",
    "        cost = mean_square(pred, Y)\n",
    "        print(\"step: %i, cost: %f, W1: %f, W2: %f, b: %f\" % (step, cost, W[0].numpy(), W[1].numpy(), b.numpy()))\n",
    "        #print(\"예측값 : {}\".format(pred.numpy()))\n",
    "        \n",
    "        '''\n",
    "        print(\"cost type: {}\".format(type(cost)))\n",
    "        print(\"W type: {}\".format(type(W)))\n",
    "        print(\"b type: {}\".format(type(b)))\n",
    "        print(\"pred type: {}\".format(type(pred)))\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 시간, 1 개월 해외거주: 679.695679 점\n",
      "1 시간, 0 개월 해외거주: 679.677734 점\n"
     ]
    }
   ],
   "source": [
    "# ======== 학습된 우리의 프로그램에 예측 문의 ========\n",
    "\n",
    "sample = np.array([\n",
    "    [3, 1] \n",
    "], dtype=np.float32)\n",
    "\n",
    "sample1 = np.array([\n",
    "    [1, 0] \n",
    "], dtype=np.float32)\n",
    "\n",
    "# 30 시간 공부하고 10개월 해외에서 거주했을 경우 토익점수 예측\n",
    "print (\"%i 시간, %i 개월 해외거주: %f 점\" % (sample[0,0], sample[0,1], linear_regression(sample).numpy()))\n",
    "print (\"%i 시간, %i 개월 해외거주: %f 점\" % (sample1[0,0], sample1[0,1], linear_regression(sample1).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
